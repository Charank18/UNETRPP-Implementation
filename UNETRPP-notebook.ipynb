{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6897390,"sourceType":"datasetVersion","datasetId":3962066},{"sourceId":7165942,"sourceType":"datasetVersion","datasetId":4139552},{"sourceId":7549906,"sourceType":"datasetVersion","datasetId":4394153},{"sourceId":7551226,"sourceType":"datasetVersion","datasetId":4343232},{"sourceId":7655456,"sourceType":"datasetVersion","datasetId":4463281},{"sourceId":7725549,"sourceType":"datasetVersion","datasetId":4462680},{"sourceId":7736551,"sourceType":"datasetVersion","datasetId":4462016},{"sourceId":7743277,"sourceType":"datasetVersion","datasetId":4491341},{"sourceId":7763437,"sourceType":"datasetVersion","datasetId":4462194},{"sourceId":7825599,"sourceType":"datasetVersion","datasetId":4585612},{"sourceId":7825686,"sourceType":"datasetVersion","datasetId":4585678},{"sourceId":7861136,"sourceType":"datasetVersion","datasetId":4611463},{"sourceId":7875421,"sourceType":"datasetVersion","datasetId":4621584},{"sourceId":7884945,"sourceType":"datasetVersion","datasetId":4628500},{"sourceId":7921470,"sourceType":"datasetVersion","datasetId":4655143},{"sourceId":7928944,"sourceType":"datasetVersion","datasetId":4660202},{"sourceId":7939502,"sourceType":"datasetVersion","datasetId":4667697},{"sourceId":7954486,"sourceType":"datasetVersion","datasetId":4678350},{"sourceId":7980823,"sourceType":"datasetVersion","datasetId":4697228}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Copyright (c) MONAI Consortium  \nLicensed under the Apache License, Version 2.0 (the \"License\");  \nyou may not use this file except in compliance with the License.  \nYou may obtain a copy of the License at  \n&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  \nUnless required by applicable law or agreed to in writing, software  \ndistributed under the License is distributed on an \"AS IS\" BASIS,  \nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \nSee the License for the specific language governing permissions and  \nlimitations under the License.\n\n# Brain tumor 3D segmentation with MONAI\n\nThis tutorial shows how to construct a training workflow of multi-labels segmentation task.\n\nAnd it contains below features:\n1. Transforms for dictionary format data.\n1. Define a new transform according to MONAI transform API.\n1. Load Nifti image with metadata, load a list of images and stack them.\n1. Randomly adjust intensity for data augmentation.\n1. Cache IO and transforms to accelerate training and validation.\n1. 3D SegResNet model, Dice loss function, Mean Dice metric for 3D segmentation task.\n1. Deterministic training for reproducibility.\n\nThe dataset comes from http://medicaldecathlon.com/.  \nTarget: Gliomas segmentation necrotic/active tumour and oedema  \nModality: Multimodal multisite MRI data (FLAIR, T1w, T1gd,T2w)  \nSize: 750 4D volumes (484 Training + 266 Testing)  \nSource: BRATS 2016 and 2017 datasets.  \nChallenge: Complex and heterogeneously-located targets\n\nBelow figure shows image patches with the tumor sub-regions that are annotated in the different modalities (top left) and the final labels for the whole dataset (right).\n(Figure taken from the [BraTS IEEE TMI paper](https://ieeexplore.ieee.org/document/6975210/))\n\n![image](../figures/brats_tasks.png)\n\nThe image patches show from left to right:\n1. the whole tumor (yellow) visible in T2-FLAIR (Fig.A).\n1. the tumor core (red) visible in T2 (Fig.B).\n1. the enhancing tumor structures (light blue) visible in T1Gd, surrounding the cystic/necrotic components of the core (green) (Fig. C).\n1. The segmentations are combined to generate the final labels of the tumor sub-regions (Fig.D): edema (yellow), non-enhancing solid core (red), necrotic/cystic core (green), enhancing core (blue).\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Project-MONAI/tutorials/blob/main/3d_segmentation/brats_segmentation_3d.ipynb)","metadata":{}},{"cell_type":"code","source":"cd \"/kaggle/input/unetr/unetr_plus_plus-main\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install batchgenerators","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install timm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install thop","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install monai","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install einops","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import nn\nfrom typing import Tuple, Union\nfrom unetr_pp.network_architecture.neural_network import SegmentationNetwork\nfrom unetr_pp.network_architecture.dynunet_block import UnetOutBlock, UnetResBlock\nfrom unetr_pp.network_architecture.tumor.model_components import UnetrPPEncoder, UnetrUpBlock\n\n\nclass UNETR_PP(SegmentationNetwork):\n    \"\"\"\n    UNETR++ based on: \"Shaker et al.,\n    UNETR++: Delving into Efficient and Accurate 3D Medical Image Segmentation\"\n    \"\"\"\n    def __init__(\n            self,\n            in_channels: int,\n            out_channels: int,\n            feature_size: int = 16,\n            hidden_size: int = 256,\n            num_heads: int = 4,\n            pos_embed: str = \"perceptron\",\n            norm_name: Union[Tuple, str] = \"instance\",\n            dropout_rate: float = 0.0,\n            depths=None,\n            dims=None,\n            conv_op=nn.Conv3d,\n            do_ds=True,\n\n    ) -> None:\n        \"\"\"\n        Args:\n            in_channels: dimension of input channels.\n            out_channels: dimension of output channels.\n            img_size: dimension of input image.\n            feature_size: dimension of network feature size.\n            hidden_size: dimensions of  the last encoder.\n            num_heads: number of attention heads.\n            pos_embed: position embedding layer type.\n            norm_name: feature normalization type and arguments.\n            dropout_rate: faction of the input units to drop.\n            depths: number of blocks for each stage.\n            dims: number of channel maps for the stages.\n            conv_op: type of convolution operation.\n            do_ds: use deep supervision to compute the loss.\n        \"\"\"\n\n        super().__init__()\n        if depths is None:\n            depths = [3, 3, 3, 3]\n        self.do_ds = do_ds\n        self.conv_op = conv_op\n        self.num_classes = out_channels\n        if not (0 <= dropout_rate <= 1):\n            raise AssertionError(\"dropout_rate should be between 0 and 1.\")\n\n        if pos_embed not in [\"conv\", \"perceptron\"]:\n            raise KeyError(f\"Position embedding layer of type {pos_embed} is not supported.\")\n\n        self.feat_size = (4, 4, 4)\n        self.hidden_size = hidden_size\n\n        self.unetr_pp_encoder = UnetrPPEncoder(dims=dims, depths=depths, num_heads=num_heads)\n\n        self.encoder1 = UnetResBlock(\n            spatial_dims=3,\n            in_channels=in_channels,\n            out_channels=feature_size,\n            kernel_size=3,\n            stride=1,\n            norm_name=norm_name,\n        )\n        self.decoder5 = UnetrUpBlock(\n            spatial_dims=3,\n            in_channels=feature_size * 16,\n            out_channels=feature_size * 8,\n            kernel_size=3,\n            upsample_kernel_size=2,\n            norm_name=norm_name,\n            out_size=8*8*8,\n        )\n        self.decoder4 = UnetrUpBlock(\n            spatial_dims=3,\n            in_channels=feature_size * 8,\n            out_channels=feature_size * 4,\n            kernel_size=3,\n            upsample_kernel_size=2,\n            norm_name=norm_name,\n            out_size=16*16*16,\n        )\n        self.decoder3 = UnetrUpBlock(\n            spatial_dims=3,\n            in_channels=feature_size * 4,\n            out_channels=feature_size * 2,\n            kernel_size=3,\n            upsample_kernel_size=2,\n            norm_name=norm_name,\n            out_size=32*32*32,\n        )\n        self.decoder2 = UnetrUpBlock(\n            spatial_dims=3,\n            in_channels=feature_size * 2,\n            out_channels=feature_size,\n            kernel_size=3,\n            upsample_kernel_size=(4, 4, 4),\n            norm_name=norm_name,\n            out_size=128*128*128,\n            conv_decoder=True,\n        )\n        self.out1 = UnetOutBlock(spatial_dims=3, in_channels=feature_size, out_channels=out_channels)\n        if self.do_ds:\n            self.out2 = UnetOutBlock(spatial_dims=3, in_channels=feature_size * 2, out_channels=out_channels)\n            self.out3 = UnetOutBlock(spatial_dims=3, in_channels=feature_size * 4, out_channels=out_channels)\n\n    def proj_feat(self, x, hidden_size, feat_size):\n        x = x.view(x.size(0), feat_size[0], feat_size[1], feat_size[2], hidden_size)\n        x = x.permute(0, 4, 1, 2, 3).contiguous()\n        return x\n\n    def forward(self, x_in):\n        #print(\"###########reached forward network\")\n        #print(\"XIN\",x_in.shape)\n        x_output, hidden_states = self.unetr_pp_encoder(x_in)\n        convBlock = self.encoder1(x_in)\n\n        # Four encoders\n        enc1 = hidden_states[0]\n        enc2 = hidden_states[1]\n        enc3 = hidden_states[2]\n        enc4 = hidden_states[3]\n\n        # Four decoders\n        dec4 = self.proj_feat(enc4, self.hidden_size, self.feat_size)\n        dec3 = self.decoder5(dec4, enc3)\n        dec2 = self.decoder4(dec3, enc2)\n        dec1 = self.decoder3(dec2, enc1)\n\n        out = self.decoder2(dec1, convBlock)\n        if self.do_ds:\n            logits = [self.out1(out), self.out2(dec1), self.out3(dec2)]\n        else:\n            logits = self.out1(out)\n\n        return logits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport einops\nmodel=UNETR_PP(in_channels=4,\n                             out_channels=3,\n                             feature_size=16,\n                             num_heads=4,\n                             depths=[3, 3, 3, 3],\n                             dims=[32, 64, 128, 256],\n                             do_ds=True,)\ninput=torch.rand(1,4,128,128,128)\n\noutput=model(input)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(output))\nprint(output[0].shape)\nprint(output[1].shape)\nprint(output[2].shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install monai","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, tqdm]\"\n!python -c \"import matplotlib\" || pip install -q matplotlib\n%matplotlib inline","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setup imports","metadata":{}},{"cell_type":"code","source":"\nfrom __future__ import annotations\n\nimport os\nimport shutil\nimport sys\nimport warnings\nfrom collections.abc import Callable, Sequence\nfrom pathlib import Path\nfrom typing import Any\n\nimport numpy as np\n\nfrom monai.apps.tcia import (\n    download_tcia_series_instance,\n    get_tcia_metadata,\n    get_tcia_ref_uid,\n    match_tcia_ref_uid_in_study,\n)\n# from monai.apps.utils import download_and_extract\nfrom monai.config.type_definitions import PathLike\nfrom monai.data import (\n    CacheDataset,\n    PydicomReader,\n    load_decathlon_datalist,\n    load_decathlon_properties,\n    partition_dataset,\n    select_cross_validation_folds,\n)\nfrom monai.transforms import LoadImaged, Randomizable\nfrom monai.utils import ensure_tuple\n\ndef _basename(p: PathLike) -> str:\n    \"\"\"get the last part of the path (removing the trailing slash if it exists)\"\"\"\n    sep = os.path.sep + (os.path.altsep or \"\") + \"/ \"\n    return Path(f\"{p}\".rstrip(sep)).name\n\ndef extractall(\n    filepath: PathLike,\n    output_dir: PathLike = \".\",\n    hash_val: str | None = None,\n    hash_type: str = \"md5\",\n    file_type: str = \"\",\n    has_base: bool = True,\n) -> None:\n    \"\"\"\n    Extract file to the output directory.\n    Expected file types are: `zip`, `tar.gz` and `tar`.\n\n    Args:\n        filepath: the file path of compressed file.\n        output_dir: target directory to save extracted files.\n        hash_val: expected hash value to validate the compressed file.\n            if None, skip hash validation.\n        hash_type: 'md5' or 'sha1', defaults to 'md5'.\n        file_type: string of file type for decompressing. Leave it empty to infer the type from the filepath basename.\n        has_base: whether the extracted files have a base folder. This flag is used when checking if the existing\n            folder is a result of `extractall`, if it is, the extraction is skipped. For example, if A.zip is unzipped\n            to folder structure `A/*.png`, this flag should be True; if B.zip is unzipped to `*.png`, this flag should\n            be False.\n\n    Raises:\n        RuntimeError: When the hash validation of the ``filepath`` compressed file fails.\n        NotImplementedError: When the ``filepath`` file extension is not one of [zip\", \"tar.gz\", \"tar\"].\n\n    \"\"\"\n    if has_base:\n        # the extracted files will be in this folder\n        cache_dir = Path(output_dir, _basename(filepath).split(\".\")[0])\n    else:\n        cache_dir = Path(output_dir)\n    if cache_dir.exists() and next(cache_dir.iterdir(), None) is not None:\n        logger.info(f\"Non-empty folder exists in {cache_dir}, skipped extracting.\")\n        return\n    filepath = Path(filepath)\n    if hash_val and not check_hash(filepath, hash_val, hash_type):\n        raise RuntimeError(\n            f\"{hash_type} check of compressed file failed: \" f\"filepath={filepath}, expected {hash_type}={hash_val}.\"\n        )\n    print(f\"Writing into directory: {output_dir}.\")\n    _file_type = file_type.lower().strip()\n    print(filepath, _file_type)\n    if filepath.name.endswith(\"zip\") or _file_type == \"zip\":\n        zip_file = zipfile.ZipFile(filepath)\n        zip_file.extractall(output_dir)\n        zip_file.close()\n        return\n    if filepath.name.endswith(\"tar\") or filepath.name.endswith(\"tar.gz\") or \"tar\" in _file_type:\n        tar_file = tarfile.open(filepath)\n        tar_file.extractall(output_dir)\n        tar_file.close()\n        return\n    raise NotImplementedError(\n        f'Unsupported file type, available options are: [\"zip\", \"tar.gz\", \"tar\"]. name={filepath} type={file_type}.'\n    )\n\n\ndef download_and_extract(\n    url: str,\n    filepath: PathLike = \"\",\n    output_dir: PathLike = \".\",\n    hash_val: str | None = None,\n    hash_type: str = \"md5\",\n    file_type: str = \"\",\n    has_base: bool = True,\n    progress: bool = True,\n) -> None:\n    \"\"\"\n    Download file from URL and extract it to the output directory.\n\n    Args:\n        url: source URL link to download file.\n        filepath: the file path of the downloaded compressed file.\n            use this option to keep the directly downloaded compressed file, to avoid further repeated downloads.\n        output_dir: target directory to save extracted files.\n            default is the current directory.\n        hash_val: expected hash value to validate the downloaded file.\n            if None, skip hash validation.\n        hash_type: 'md5' or 'sha1', defaults to 'md5'.\n        file_type: string of file type for decompressing. Leave it empty to infer the type from url's base file name.\n        has_base: whether the extracted files have a base folder. This flag is used when checking if the existing\n            folder is a result of `extractall`, if it is, the extraction is skipped. For example, if A.zip is unzipped\n            to folder structure `A/*.png`, this flag should be True; if B.zip is unzipped to `*.png`, this flag should\n            be False.\n        progress: whether to display progress bar.\n    \"\"\"\n    print()\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        filename = filepath or Path(tmp_dir, _basename(url)).resolve()\n#         shutil.copy(\"/kaggle/input/segresnet-data/BraTS-MEN-Train.zip\", tmp_dir)\n#         download_url(url=url, filepath=filename, hash_val=hash_val, hash_type=hash_type, progress=progress)\n        extractall(filepath=filename, output_dir=output_dir, file_type=file_type, has_base=has_base)\n        \nclass DecathlonDataset(Randomizable, CacheDataset):\n    \"\"\"\n    The Dataset to automatically download the data of Medical Segmentation Decathlon challenge\n    (http://medicaldecathlon.com/) and generate items for training, validation or test.\n    It will also load these properties from the JSON config file of dataset. user can call `get_properties()`\n    to get specified properties or all the properties loaded.\n    It's based on :py:class:`monai.data.CacheDataset` to accelerate the training process.\n\n    Args:\n        root_dir: user's local directory for caching and loading the MSD datasets.\n        task: which task to download and execute: one of list (\"Task01_BrainTumour\", \"Task02_Heart\",\n            \"Task03_Liver\", \"Task04_Hippocampus\", \"Task05_Prostate\", \"Task06_Lung\", \"Task07_Pancreas\",\n            \"Task08_HepaticVessel\", \"Task09_Spleen\", \"Task10_Colon\").\n        section: expected data section, can be: `training`, `validation` or `test`.\n        transform: transforms to execute operations on input data.\n            for further usage, use `EnsureChannelFirstd` to convert the shape to [C, H, W, D].\n        download: whether to download and extract the Decathlon from resource link, default is False.\n            if expected file already exists, skip downloading even set it to True.\n            user can manually copy tar file or dataset folder to the root directory.\n        val_frac: percentage of validation fraction in the whole dataset, default is 0.2.\n        seed: random seed to randomly shuffle the datalist before splitting into training and validation, default is 0.\n            note to set same seed for `training` and `validation` sections.\n        cache_num: number of items to be cached. Default is `sys.maxsize`.\n            will take the minimum of (cache_num, data_length x cache_rate, data_length).\n        cache_rate: percentage of cached data in total, default is 1.0 (cache all).\n            will take the minimum of (cache_num, data_length x cache_rate, data_length).\n        num_workers: the number of worker threads if computing cache in the initialization.\n            If num_workers is None then the number returned by os.cpu_count() is used.\n            If a value less than 1 is specified, 1 will be used instead.\n        progress: whether to display a progress bar when downloading dataset and computing the transform cache content.\n        copy_cache: whether to `deepcopy` the cache content before applying the random transforms,\n            default to `True`. if the random transforms don't modify the cached content\n            (for example, randomly crop from the cached image and deepcopy the crop region)\n            or if every cache item is only used once in a `multi-processing` environment,\n            may set `copy=False` for better performance.\n        as_contiguous: whether to convert the cached NumPy array or PyTorch tensor to be contiguous.\n            it may help improve the performance of following logic.\n        runtime_cache: whether to compute cache at the runtime, default to `False` to prepare\n            the cache content at initialization. See: :py:class:`monai.data.CacheDataset`.\n\n    Raises:\n        ValueError: When ``root_dir`` is not a directory.\n        ValueError: When ``task`` is not one of [\"Task01_BrainTumour\", \"Task02_Heart\",\n            \"Task03_Liver\", \"Task04_Hippocampus\", \"Task05_Prostate\", \"Task06_Lung\", \"Task07_Pancreas\",\n            \"Task08_HepaticVessel\", \"Task09_Spleen\", \"Task10_Colon\"].\n        RuntimeError: When ``dataset_dir`` doesn't exist and downloading is not selected (``download=False``).\n\n    Example::\n\n        transform = Compose(\n            [\n                LoadImaged(keys=[\"image\", \"label\"]),\n                EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n                ScaleIntensityd(keys=\"image\"),\n                ToTensord(keys=[\"image\", \"label\"]),\n            ]\n        )\n\n        val_data = DecathlonDataset(\n            root_dir=\"./\", task=\"Task09_Spleen\", transform=transform, section=\"validation\", seed=12345, download=True\n        )\n\n        print(val_data[0][\"image\"], val_data[0][\"label\"])\n\n    \"\"\"\n\n    resource = {\n        \"Task01_BrainTumour\": \"/kaggle/input/segresnet-data/BraTS-MEN-Train\",\n        \"Task02_Heart\": \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task02_Heart.tar\",\n        \"Task03_Liver\": \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task03_Liver.tar\",\n        \"Task04_Hippocampus\": \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task04_Hippocampus.tar\",\n        \"Task05_Prostate\": \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task05_Prostate.tar\",\n        \"Task06_Lung\": \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task06_Lung.tar\",\n        \"Task07_Pancreas\": \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task07_Pancreas.tar\",\n        \"Task08_HepaticVessel\": \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task08_HepaticVessel.tar\",\n        \"Task09_Spleen\": \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task09_Spleen.tar\",\n        \"Task10_Colon\": \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task10_Colon.tar\",\n    }\n    md5 = {\n        \"Task01_BrainTumour\": \"240a19d752f0d9e9101544901065d872\",\n        \"Task02_Heart\": \"06ee59366e1e5124267b774dbd654057\",\n        \"Task03_Liver\": \"a90ec6c4aa7f6a3d087205e23d4e6397\",\n        \"Task04_Hippocampus\": \"9d24dba78a72977dbd1d2e110310f31b\",\n        \"Task05_Prostate\": \"35138f08b1efaef89d7424d2bcc928db\",\n        \"Task06_Lung\": \"8afd997733c7fc0432f71255ba4e52dc\",\n        \"Task07_Pancreas\": \"4f7080cfca169fa8066d17ce6eb061e4\",\n        \"Task08_HepaticVessel\": \"641d79e80ec66453921d997fbf12a29c\",\n        \"Task09_Spleen\": \"410d4a301da4e5b2f6f86ec3ddba524e\",\n        \"Task10_Colon\": \"bad7a188931dc2f6acf72b08eb6202d0\",\n    }\n\n    def __init__(\n        self,\n        root_dir: PathLike,\n        task: str,\n        section: str,\n        transform: Sequence[Callable] | Callable = (),\n        download: bool = False,\n        seed: int = 0,\n        val_frac: float = 0.2,\n        cache_num: int = sys.maxsize,\n        cache_rate: float = 1.0,\n        num_workers: int = 1,\n        progress: bool = True,\n        copy_cache: bool = True,\n        as_contiguous: bool = True,\n        runtime_cache: bool = False,\n    ) -> None:\n        root_dir = Path(root_dir)\n        if not root_dir.is_dir():\n            raise ValueError(\"Root directory root_dir must be a directory.\")\n        self.section = section\n        self.val_frac = val_frac\n        self.set_random_state(seed=seed)\n        if task not in self.resource:\n            raise ValueError(f\"Unsupported task: {task}, available options are: {list(self.resource.keys())}.\")\n        dataset_dir = root_dir / task\n#         tarfile_name = f\"{dataset_dir}.tar\"\n#         if download:\n#             download_and_extract(\n#                 url=self.resource[task],\n#                 filepath=tarfile_name,\n#                 output_dir=root_dir,\n#                 hash_val=self.md5[task],\n#                 hash_type=\"md5\",\n#                 progress=progress,\n#             )\n\n#         if not dataset_dir.exists():\n#             raise RuntimeError(\n#                 f\"Cannot find dataset directory: {dataset_dir}, please use download=True to download it.\"\n#             )\n#         dataset_dir = \"/kaggle/input/meningits-part1/brain-men-train1\"\n        self.indices: np.ndarray = np.array([])\n        data = self._generate_data_list(\"/kaggle/input/segres-json\")\n        # as `release` key has typo in Task04 config file, ignore it.\n        property_keys = [\n            \"name\",\n            \"description\",\n            \"reference\",\n            \"licence\",\n            \"tensorImageSize\",\n            \"modality\",\n            \"labels\",\n            \"numTraining\",\n            \"numTest\",\n        ]\n#         self._properties = load_decathlon_properties(\"/kaggle/input/segres-json/dataset.json\", property_keys)\n        if transform == ():\n            transform = LoadImaged([\"image\", \"label\"])\n        CacheDataset.__init__(\n            self,\n            data=data,\n            transform=transform,\n            cache_num=cache_num,\n            cache_rate=cache_rate,\n            num_workers=num_workers,\n            progress=progress,\n            copy_cache=copy_cache,\n            as_contiguous=as_contiguous,\n            runtime_cache=runtime_cache,\n        )\n\n\n# [docs]\n    def get_indices(self) -> np.ndarray:\n        \"\"\"\n        Get the indices of datalist used in this dataset.\n\n        \"\"\"\n        return self.indices\n\n\n\n\n# [docs]\n    def randomize(self, data: np.ndarray) -> None:\n        self.R.shuffle(data)\n\n\n\n\n# [docs]\n    def get_properties(self, keys: Sequence[str] | str | None = None) -> dict:\n        \"\"\"\n        Get the loaded properties of dataset with specified keys.\n        If no keys specified, return all the loaded properties.\n\n        \"\"\"\n        if keys is None:\n            return self._properties\n        if self._properties is not None:\n            return {key: self._properties[key] for key in ensure_tuple(keys)}\n        return {}\n\n\n\n    def _generate_data_list(self, dataset_dir: PathLike) -> list[dict]:\n        # the types of the item in data list should be compatible with the dataloader\n        dataset_dir = Path(dataset_dir) \n        section = \"training\" if self.section in [\"training\", \"validation\", \"test\"] else \"test\"\n        datalist = load_decathlon_datalist(\"/kaggle/input/segres-json/dataset (1).json\", True, section)\n#         datalist2 = load_decathlon_datalist(\"/kaggle/input/segres-json/dataset.json\", True, section)\n#         datalist = datalist.append(datalist2)\n        print(datalist[898])\n        print(\".............................................................................\")\n        return self._split_datalist(datalist)\n\n    def _split_datalist(self, datalist: list[dict]) -> list[dict]:\n#         if self.section == \"test\":\n#             return datalist\n        length = len(datalist)\n        indices = np.arange(length)\n        self.randomize(indices)\n\n        \n        val_length = int(length * self.val_frac)\n        \n        if self.section == \"training\":\n            self.indices = indices[val_length:]\n        elif self.section == \"validation\":\n            self.indices = indices[:100]\n        else:\n            self.indices = indices[100:246]\n        print(self.indices)\n        return [datalist[i] for i in self.indices]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport shutil\nimport tempfile\nimport time\nimport matplotlib.pyplot as plt\n# from monai.apps import DecathlonDataset\nfrom monai.config import print_config\nfrom monai.data import DataLoader, decollate_batch\nfrom monai.handlers.utils import from_engine\nfrom monai.losses import DiceLoss\nfrom monai.inferers import sliding_window_inference\nfrom monai.metrics import DiceMetric\n# from monai.networks.nets import SegResNet\nfrom monai.transforms import (\n    Activations,\n    Activationsd,\n    AsDiscrete,\n    AsDiscreted,\n    Compose,\n    Invertd,\n    LoadImaged,\n    MapTransform,\n    NormalizeIntensityd,\n    Orientationd,\n    RandFlipd,\n    RandScaleIntensityd,\n    RandShiftIntensityd,\n    RandSpatialCropd,\n    Spacingd,\n    EnsureTyped,\n    EnsureChannelFirstd,\n)\nfrom monai.utils import set_determinism\n\nimport torch\n\nprint_config()","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setup data directory\n\nYou can specify a directory with the `MONAI_DATA_DIRECTORY` environment variable.  \nThis allows you to save results and reuse downloads.  \nIf not specified a temporary directory will be used.","metadata":{}},{"cell_type":"code","source":"directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\nroot_dir = tempfile.mkdtemp() if directory is None else directory\nprint(root_dir)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set deterministic training for reproducibility","metadata":{}},{"cell_type":"code","source":"set_determinism(seed=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ConvertToMultiChannelBasedOnBratsClassesd(MapTransform):\n    \"\"\"\n    Convert labels to multi channels based on brats classes:\n    label 1 is the peritumoral edema\n    label 2 is the GD-enhancing tumor\n    label 3 is the necrotic and non-enhancing tumor core\n    The possible classes are TC (Tumor core), WT (Whole tumor)\n    and ET (Enhancing tumor).\n\n    \"\"\"\n\n    def __call__(self, data):\n        d = dict(data)\n        for key in self.keys:\n            result = []\n            # merge label 2 and label 3 to construct TC\n            result.append(torch.logical_or(d[key] == 2, d[key] == 3))\n            # merge labels 1, 2 and 3 to construct WT\n            result.append(torch.logical_or(torch.logical_or(d[key] == 2, d[key] == 3), d[key] == 1))\n            # label 2 is ET\n            result.append(d[key] == 2)\n            d[key] = torch.stack(result, axis=0).float()\n        return d","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transform = Compose(\n    [\n        # load 4 Nifti images and stack them together\n        LoadImaged(keys=[\"image\", \"label\"]),\n        EnsureChannelFirstd(keys=\"image\"),\n        EnsureTyped(keys=[\"image\", \"label\"]),\n        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n        Spacingd(\n            keys=[\"image\", \"label\"],\n            pixdim=(1.0, 1.0, 1.0),\n            mode=(\"bilinear\", \"nearest\"),\n        ),\n        RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=[128, 128, 128], random_size=False),\n        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n        RandScaleIntensityd(keys=\"image\", factors=0.1, prob=1.0),\n        RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=1.0),\n        \n    ]\n)\nval_transform = Compose(\n    [\n        LoadImaged(keys=[\"image\", \"label\"]),\n        EnsureChannelFirstd(keys=\"image\"),\n        EnsureTyped(keys=[\"image\", \"label\"]),\n        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n        Spacingd(\n            keys=[\"image\", \"label\"],\n            pixdim=(1.0, 1.0, 1.0),\n            mode=(\"bilinear\", \"nearest\"),\n        ),\n        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n    ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define a new transform to convert brain tumor labels\n\nHere we convert the multi-classes labels into multi-labels segmentation task in One-Hot format.","metadata":{}},{"cell_type":"markdown","source":"## Setup transforms for training and validation","metadata":{}},{"cell_type":"markdown","source":"## Quickly load data with DecathlonDataset\n\nHere we use `DecathlonDataset` to automatically download and extract the dataset.\nIt inherits MONAI `CacheDataset`, if you want to use less memory, you can set `cache_num=N` to cache N items for training and use the default args to cache all the items for validation, it depends on your memory size.","metadata":{}},{"cell_type":"code","source":"# here we don't cache any data in case out of memory issue\ntrain_ds = DecathlonDataset(\n    root_dir=root_dir,\n    task=\"Task01_BrainTumour\",\n    transform=train_transform,\n    section=\"training\",\n    download=True,\n    cache_rate=0.0,\n    num_workers=4,\n)\ntrain_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=4)\n\nval_ds = DecathlonDataset(\n    root_dir=root_dir,\n    task=\"Task01_BrainTumour\",\n    transform=val_transform,\n    section=\"validation\",\n    download=False,\n    cache_rate=0.0,\n    num_workers=4,\n)\nval_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=4)\n\ntest_ds = DecathlonDataset(\n    root_dir=root_dir,\n    task=\"Task01_BrainTumour\",\n    transform=val_transform,\n    section=\"test\",\n    download=False,\n    cache_rate=0.0,\n    num_workers=4,\n)\ntest_loader = DataLoader(test_ds, batch_size=1, shuffle=False, num_workers=4)\n\nprint(len(train_ds), len(val_ds), len(test_ds))","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check data shape and visualize","metadata":{}},{"cell_type":"code","source":"# pick one image from DecathlonDataset to visualize and check the 4 channels\nval_data_example = val_ds[2]\nprint(f\"image shape: {val_data_example['image'].shape}\")\nplt.figure(\"image\", (24, 6))\nfor i in range(4):\n    plt.subplot(1, 4, i + 1)\n    plt.title(f\"image channel {i}\")\n    plt.imshow(val_data_example[\"image\"][i, :, :, 60].detach().cpu(), cmap=\"gray\")\nplt.show()\n# also visualize the 3 channels label corresponding to this image\nprint(f\"label shape: {val_data_example['label'].shape}\")\nplt.figure(\"label\", (18, 6))\nfor i in range(3):\n    plt.subplot(1, 3, i + 1)\n    plt.title(f\"label channel {i}\")\n    plt.imshow(val_data_example[\"label\"][i, :, :, 60].detach().cpu())\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Model, Loss, Optimizer","metadata":{}},{"cell_type":"code","source":"!pip install thop","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from thop import profile\nfrom thop import clever_format","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_epochs = 100\nval_interval = 1\nVAL_AMP = True\n\n\ndevice = torch.device(\"cuda:0\")\nmodel = UNETR_PP(in_channels=4,out_channels=3,feature_size=16,num_heads=4,depths=[3, 3, 3, 3],dims=[32, 64, 128, 256],do_ds=True).to(device)\nprint(model)\nimage_size = 128\ninputs = torch.rand(1, 4, image_size, image_size, image_size).to(device)\ninput_size = (1, 4, 128, 128, 128)\n# input_tensor = torch.randn(*input_size).to(device)\n\n# flops, params = profile(model, inputs=(input_tensor,))\n\n# # Convert FLOPs to gigaFLOPs and format the results\n# flops, params = clever_format([flops, params], \"%.2f\")\n# print(f\"FLOPs: {flops}, Params: {params}\")\n\noutputs = model(inputs)\nprint(outputs[0].shape)\n# print(model)\n# model.load_state_dict(torch.load(\"/kaggle/input/segseresnet-checkpoints/SegSeResNet_best_model_198.pth\"))\n\nloss_function = DiceLoss(smooth_nr=0, smooth_dr=1e-5, squared_pred=True, to_onehot_y=False, sigmoid=True)\noptimizer = torch.optim.Adam(model.parameters(), 1e-4, weight_decay=1e-5)\nlr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n\ndice_metric = DiceMetric(include_background=True, reduction=\"mean\")\ndice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\")\n\npost_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n\n\n# define inference method\ndef inference(input):\n    def _compute(input):\n        return sliding_window_inference(\n            inputs=input,\n            roi_size=(128, 128, 128),\n            sw_batch_size=1,\n            predictor=model,\n            overlap=0.5,\n        )\n\n    if VAL_AMP:\n        with torch.cuda.amp.autocast():\n            return _compute(input)\n    else:\n        return _compute(input)\n\n\n# use amp to accelerate training\nscaler = torch.cuda.amp.GradScaler()\n# enable cuDNN benchmark\ntorch.backends.cudnn.benchmark = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Execute a typical PyTorch training process","metadata":{}},{"cell_type":"code","source":"best_metric = -1\nbest_metric_epoch = -1\nbest_metrics_epochs_and_time = [[], [], []]\nepoch_loss_values = []\nval_epoch_loss_values = []\nmetric_values = []\ntrain_metric_values = []\nmetric_values_tc = []\nmetric_values_wt = []\nmetric_values_et = []\n\nstart_epoch = 0\nlatest_checkpoint_path = \"/kaggle/input/best-metric-model-77/best_metric_model_77.pth\"\nif os.path.exists(latest_checkpoint_path):\n    checkpoint = torch.load(latest_checkpoint_path)\n    model.load_state_dict(checkpoint)\n    start_epoch = 80\n    print(\"checkpoint loaded\")\nelse:\n    print(\"No checkpoint. Starting from scratch\")\n\ntotal_start = time.time()\n\nfor epoch in range(start_epoch, max_epochs):\n    epoch_start = time.time()\n    print(\"-\" * 10)\n    print(f\"epoch {epoch + 1}/{max_epochs}\")\n    model.train()\n    epoch_loss = 0\n    epoch_metric = 0\n    val_epoch_loss = 0\n    step = 0\n    for batch_data in train_loader:\n        step_start = time.time()\n        step += 1\n        inputs, labels = (\n            batch_data[\"image\"].to(device),\n            batch_data[\"label\"].to(device),\n        )\n        optimizer.zero_grad()\n        with torch.cuda.amp.autocast():\n            outputs = model(inputs)\n            loss = loss_function(outputs[0], labels)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        epoch_loss += loss.item()\n        outputs = [post_trans(i) for i in decollate_batch(outputs[0])]\n        dice_metric(y_pred=outputs, y=labels)\n        train_metric = dice_metric.aggregate().item()\n        epoch_metric += train_metric\n        \n#         dice_metric_batch(y_pred=outputs, y=labels)\n        print(\n            f\"{step}/{len(train_ds) // train_loader.batch_size}\"\n            f\", train_loss: {loss.item():.4f}\"\n            f\", current mean dice: {train_metric:.4f}\"\n            f\", step time: {(time.time() - step_start):.4f}\"\n        )\n    lr_scheduler.step()\n    epoch_loss /= step\n    epoch_metric /= step\n    epoch_loss_values.append(epoch_loss)\n    train_metric_values.append(epoch_metric)\n    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\"\n          f\", average metric: {epoch_metric:.4f}\")\n    print(\"train_time: \", time.time()-epoch_start)\n\n    if (epoch + 1) % val_interval == 0:\n        model.eval()\n        val_start = time.time()\n        with torch.no_grad():\n            val_step = 0\n            for val_data in val_loader:\n                val_step += 1\n                val_inputs, val_labels = (\n                    val_data[\"image\"].to(device),\n                    val_data[\"label\"].to(device),\n                )\n                val_outputs = inference(val_inputs)\n                loss = loss_function(val_outputs[0], val_labels)\n                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs[0])]\n                dice_metric(y_pred=val_outputs, y=val_labels)\n                dice_metric_batch(y_pred=val_outputs, y=val_labels)\n                val_epoch_loss += loss.item()\n                \n            metric = dice_metric.aggregate().item()\n            metric_values.append(metric)\n            metric_batch = dice_metric_batch.aggregate()\n            metric_tc = metric_batch[0].item()\n            metric_values_tc.append(metric_tc)\n            metric_wt = metric_batch[1].item()\n            metric_values_wt.append(metric_wt)\n            metric_et = metric_batch[2].item()\n            metric_values_et.append(metric_et)\n            dice_metric.reset()\n            dice_metric_batch.reset()\n\n            if metric > best_metric:\n                best_metric = metric\n                best_metric_epoch = epoch + 1\n                best_metrics_epochs_and_time[0].append(best_metric)\n                best_metrics_epochs_and_time[1].append(best_metric_epoch)\n                best_metrics_epochs_and_time[2].append(time.time() - total_start)\n                torch.save(\n                    model.state_dict(),\n                    os.path.join(\"/kaggle/working/\", f\"best_metric_model_{epoch+1}.pth\"),\n                )\n                print(\"saved new best metric model\")\n            chk_file_name = \"epoch_\" + str(epoch+1) + \"_model.pth\"    \n            torch.save(\n                    model.state_dict(),\n                    os.path.join(\"/kaggle/working/\", chk_file_name),\n                )\n            val_epoch_loss /= val_step\n            val_epoch_loss_values.append(val_epoch_loss)\n            print(\n                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n                f\", val_loss: {val_epoch_loss:.4f}\"\n                f\" tc: {metric_tc:.4f} wt: {metric_wt:.4f} et: {metric_et:.4f}\"\n                f\"\\nbest mean dice: {best_metric:.4f}\"\n                f\" at epoch: {best_metric_epoch}\"\n            )\n        print(\"val_time: \", time.time()-val_start)\n    print(f\"time consuming of epoch {epoch + 1} is: {(time.time() - epoch_start):.4f}\")\ntotal_time = time.time() - total_start","metadata":{"scrolled":true,"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(val_epoch_loss_values), val_epoch_loss_values)\nprint(len(epoch_loss_values), epoch_loss_values)\nprint(len(train_metric_values), train_metric_values)\nprint(len(metric_values), metric_values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\n\nsource_path = \"/kaggle/input/metrics-loss/SegResNet_data.csv\"\ndestination_path = \"/kaggle/working/SegResNet_data_3.csv\"\n\n\n# Copy the file\nshutil.copy(source_path, destination_path)\n\nprint(f\"File '{source_path}' copied to '{destination_path}'.\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\n\n# Your lists of data\n\n# list1 = [1, 2, 3, 4, 5]\n# list2 = ['a', 'b', 'c', 'd', 'e']\n# list3 = [10, 20, 30, 40, 50]\n# list4 = ['apple', 'banana', 'cherry', 'date', 'elderberry']\n\n# CSV file path\ncsv_file_path = '/kaggle/working/SegResNet_data_3.csv'\n\n# Column names\nfieldnames = ['epoch_loss_values', 'train_metric_values', 'val_epoch_loss_values', 'metric_values']\n\n# Writing lists to a CSV file with specific column names\nwith open(csv_file_path, 'a', newline='') as csvfile:  # Change 'w' to 'a' for append mode\n    # Create a CSV writer object with DictWriter\n    csv_writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n    # Write the lists to the specified columns\n    for i in range(len(epoch_loss_values)):\n        csv_writer.writerow({\n            'epoch_loss_values': epoch_loss_values[i],\n            'train_metric_values': train_metric_values[i],\n            'val_epoch_loss_values': val_epoch_loss_values[i],\n            'metric_values': metric_values[i]\n        })\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {16}, total time: {1}.\")","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot the loss and metric","metadata":{}},{"cell_type":"code","source":"plt.figure(\"train\", (12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Epoch Average Loss\")\nx = [i + 1 for i in range(len(epoch_loss_values))]\ny = epoch_loss_values\nplt.xlabel(\"epoch\")\nplt.plot(x, y, color=\"red\")\nplt.subplot(1, 2, 2)\nplt.title(\"Val Mean Dice\")\nx = [val_interval * (i + 1) for i in range(len(metric_values))]\ny = metric_values\nplt.xlabel(\"epoch\")\nplt.plot(x, y, color=\"green\")\nplt.show()\n\nplt.figure(\"train\", (18, 6))\nplt.subplot(1, 3, 1)\nplt.title(\"Val Mean Dice TC\")\nx = [val_interval * (i + 1) for i in range(len(metric_values_tc))]\ny = metric_values_tc\nplt.xlabel(\"epoch\")\nplt.plot(x, y, color=\"blue\")\nplt.subplot(1, 3, 2)\nplt.title(\"Val Mean Dice WT\")\nx = [val_interval * (i + 1) for i in range(len(metric_values_wt))]\ny = metric_values_wt\nplt.xlabel(\"epoch\")\nplt.plot(x, y, color=\"brown\")\nplt.subplot(1, 3, 3)\nplt.title(\"Val Mean Dice ET\")\nx = [val_interval * (i + 1) for i in range(len(metric_values_et))]\ny = metric_values_et\nplt.xlabel(\"epoch\")\nplt.plot(x, y, color=\"purple\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check best model output with the input image and label","metadata":{}},{"cell_type":"code","source":"pip install nibabel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nibabel as nib \nimg_add = \"/kaggle/input/trainingdata-brats-part2/BraTS-GLI-00814-000/BraTS-GLI-00814-000-t1c.nii\"\nlabel_add = \"/kaggle/input/trainingdata-brats-part2/BraTS-GLI-00814-000/BraTS-GLI-00814-000-t1n.nii\"\nimg_add_2 = \"/kaggle/input/trainingdata-brats-part2/BraTS-GLI-00814-000/BraTS-GLI-00814-000-t2f.nii\"\nimg_add_3 = \"/kaggle/input/trainingdata-brats-part2/BraTS-GLI-00814-000/BraTS-GLI-00814-000-t2w.nii\"\nimg = nib.load(img_add).get_fdata()\nlabel = nib.load(label_add).get_fdata()\nimg_2 = nib.load(img_add_2).get_fdata()\nimg_3 = nib.load(img_add_3).get_fdata()\nprint(f\"image shape: {img.shape}, label shape: {label.shape}\")\nplt.figure(\"image\", (18, 6))\nplt.subplot(1, 4, 1)\nplt.title(\"BraTS-GLI-00814-000-t1c.nii\")\nplt.imshow(img[:, :, 78], cmap=\"gray\")\nplt.subplot(1, 4, 2)\nplt.title(\"BraTS-GLI-00814-000-t1n.nii\")\nplt.imshow(label[:, :, 78], cmap=\"gray\")\nplt.subplot(1, 4, 3)\nplt.title(\"BraTS-GLI-00814-000-t2f.nii\")\nplt.imshow(img_2[:, :, 78], cmap=\"gray\")\nplt.subplot(1, 4, 4)\nplt.title(\"BraTS-GLI-00814-000-t2w.nii\")\nplt.imshow(img_3[:, :, 78], cmap=\"gray\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nibabel as nib \nfrom matplotlib.colors import ListedColormap\nimg_add = \"/kaggle/input/trainingdata-brats-part2/BraTS-GLI-00814-000/BraTS-GLI-00814-000-t2f.nii\"\nlabel_add = \"/kaggle/input/trainingdata-brats-part2/BraTS-GLI-00814-000/BraTS-GLI-00814-000-seg.nii\"\nimg = nib.load(img_add).get_fdata()\nlabel = nib.load(label_add).get_fdata()\n# Create a custom colormap with purple, blue, and orange colors\ncmap = ListedColormap(['black', 'blue', 'orange', 'green'])\n\nprint(f\"image shape: {img.shape}, label shape: {label.shape}\")\nplt.figure(\"image\", (18, 6))\nplt.subplot(1, 3, 1)\nplt.title(\"BraTS-GLI-00000-000-t2f.nii\")\nplt.imshow(img[:, :, 78], cmap=\"gray\")\nplt.subplot(1, 3, 2)\nplt.title(\"BraTS-GLI-00000-000-seg.nii\")\nplt.imshow(label[:, :, 78])\nplt.subplot(1, 3, 3)\nplt.title(\"BraTS-GLI-00000-000-seg.nii\")\nplt.imshow(label[:, :, 78], cmap = cmap)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"/kaggle/input/97model\"))\ncmap = ListedColormap(['black', 'blue', 'red', 'black'])\nmodel.eval()\nwith torch.no_grad():\n    # select one image to evaluate and visualize the model output\n    val_input = val_ds[1][\"image\"].unsqueeze(0).to(device)\n    roi_size = (128, 128, 128)\n    sw_batch_size = 4\n    val_output = inference(val_input)\n    val_output = post_trans(val_output[0])\n    titles_modalities = [\"t1c\", \"t1n\", \"t2f\", \"t2w\"]\n    titles = [\"Tumor Core (TC) i/p Channel\", \"Whole Tumor (WT) i/p Channel\", \"Enhancing tumor (ET) i/p Channel\"]\n    plt.figure(\"image\", (24, 6))\n    for i in range(4):\n        plt.subplot(1, 4, i + 1)\n        plt.title(titles_modalities[i])\n        plt.imshow(val_ds[1][\"image\"][i, :, :, 78].detach().cpu(), cmap=\"gray\")\n    plt.show()\n#     # visualize the 3 channels label corresponding to this image\n#     plt.figure(\"label\", (18, 6))\n#     for i in range(3):\n#         plt.subplot(1, 3, i + 1)\n#         plt.title(titles[i])\n#         plt.imshow(val_ds[1][\"label\"][i, :, :, 78].detach().cpu())\n#     plt.show()\n#     # visualize the 3 channels model output corresponding to this image\n#     plt.figure(\"output\", (18, 6))\n#     for i in range(3):\n#         plt.subplot(1, 3, i + 1)\n#         plt.title(f\"output channel {i}\")\n#         plt.imshow(val_output[i, :, :, 78].detach().cpu())\n#     plt.show()\n    \n    seg_label = torch.zeros((val_ds[1][\"label\"].shape[1], val_ds[1][\"label\"].shape[2], val_ds[1][\"label\"].shape[3]))\n    seg_label[val_ds[1][\"label\"][1] == 1] = 2\n    seg_label[val_ds[1][\"label\"][0] == 1] = 1\n    seg_label[val_ds[1][\"label\"][2] == 1] = 4\n    \n    seg_out = torch.zeros((val_output.shape[1], val_output.shape[2], val_output.shape[3]))\n    seg_out[val_output[1] == 1] = 2\n    seg_out[val_output[0] == 1] = 1\n    seg_out[val_output[2] == 1] = 4\n    \n    slice_num = 78\n#     img_add = \"/kaggle/input/trainingdata-brats-part2/BraTS-GLI-00742-000/BraTS-GLI-00742-000-t2w.nii\",\n#     label_add = \"/kaggle/input/trainingdata-brats-part2/BraTS-GLI-00742-000/BraTS-GLI-00742-000-seg.nii\",\n#     img = nib.load(img_add).get_fdata()\n#     label = nib.load(label_add).get_fdata()\n#     plt.figure(\"image\", (18, 6))\n# #     plt.subplot(1, 3, 1)\n# #     plt.title(\"image\")\n# #     plt.imshow(seg_label[:, :, slice_num])\n#     plt.subplot(1, 2, 1)\n#     plt.title(\"Ground Truth Mask\")\n#     plt.imshow(seg_label[:, :, slice_num])\n#     plt.subplot(1, 2, 2)\n#     plt.title(\"Predicted Mask\")\n#     plt.imshow(seg_out[:, :, slice_num])\n#     plt.show()\n    \n    plt.figure(\"image\", (18, 6))\n    for i in range(3):\n        plt.subplot(1, 4, i + 1)\n        plt.title(titles[i])\n        plt.imshow(val_ds[1][\"label\"][i, :, :, 78].detach().cpu())\n    plt.subplot(1, 4, 4)\n    plt.title(\"Ground Truth Mask\")\n    plt.imshow(seg_label[:, :, slice_num])\n    plt.show()   \n    titles_output = [\"Tumor Core (TC) o/p Channel\", \"Whole Tumor (WT) o/p Channel\", \"Enhancing tumor (ET) o/p Channel\"]\n    plt.figure(\"image\", (18, 6))\n    for i in range(3):\n        plt.subplot(1, 4, i + 1)\n        plt.title(titles_output[i])\n        plt.imshow(val_output[i, :, :, 78].detach().cpu())\n    plt.subplot(1, 4, 4)\n    plt.title(\"Predicted Mask\")\n    plt.imshow(seg_out[:, :, slice_num])\n    plt.show()   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation on original image spacings","metadata":{}},{"cell_type":"code","source":"val_org_transforms = Compose(\n    [\n        LoadImaged(keys=[\"image\", \"label\"]),\n        EnsureChannelFirstd(keys=[\"image\"]),\n        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n        Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n        Spacingd(keys=[\"image\"], pixdim=(1.0, 1.0, 1.0), mode=\"bilinear\"),\n        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n    ]\n)\n\npost_transforms = Compose(\n    [\n        Invertd(\n            keys=\"pred\",\n            transform=val_transform,\n            orig_keys=\"image\",\n            meta_keys=\"pred_meta_dict\",\n            orig_meta_keys=\"image_meta_dict\",\n            meta_key_postfix=\"meta_dict\",\n            nearest_interp=False,\n            to_tensor=True,\n            device=\"cpu\",\n        ),\n        Activationsd(keys=\"pred\", sigmoid=True),\n        AsDiscreted(keys=\"pred\", threshold=0.5),\n    ]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from monai.metrics import HausdorffDistanceMetric\nfrom monai.metrics import SurfaceDiceMetric\nfrom monai.metrics import MeanIoU\nfrom monai.metrics import SurfaceDistanceMetric\nfrom monai.metrics import ROCAUCMetric\nfrom monai.metrics import SurfaceDiceMetric\n\nhd_metric = HausdorffDistanceMetric(include_background=False, reduction=\"mean\", percentile = 95)\nhd_metric_batch = HausdorffDistanceMetric(include_background=True, reduction=\"mean_batch\", percentile = 95)\n\nsd_metric = SurfaceDistanceMetric(include_background=True, reduction=\"mean\")\nsd_metric_batch = SurfaceDistanceMetric(include_background=True, reduction=\"mean_batch\")\n\nmeanIoU_metric = MeanIoU(include_background=True, reduction=\"mean\")\nmeanIoU_metric_batch = MeanIoU(include_background=True, reduction=\"mean_batch\")\n\nsurfaceDice_metric = SurfaceDiceMetric(include_background=True, reduction=\"mean\", class_thresholds = (0.01, 0.01, 0.01))\nsurfaceDice_metric_batch = SurfaceDiceMetric(include_background=True, reduction=\"mean_batch\", class_thresholds = (0.01, 0.01, 0.01))\n\nmodel.load_state_dict(torch.load(\"/kaggle/input/97model/best_metric_model_97.pth\"))\nmodel.eval()\ncount = 0\nwith torch.no_grad():\n    infer_start = time.time()\n    for val_data in test_loader:\n        count = count + 1\n        print(count)\n#         val_inputs = val_data[\"image\"].to(device)\n#         val_data[\"pred\"] = inference(val_inputs)\n#         val_data = [post_trans(i) for i in decollate_batch(val_data)]\n#         val_outputs, val_labels = from_engine([\"pred\", \"label\"])(val_data)\n        val_inputs, val_labels = (\n            val_data[\"image\"].to(device),\n            val_data[\"label\"].to(device),\n        )\n        val_outputs = inference(val_inputs)\n        val_outputs = [post_trans(i) for i in decollate_batch(val_outputs[0])]\n        \n        dice_metric(y_pred=val_outputs, y=val_labels)\n        dice_metric_batch(y_pred=val_outputs, y=val_labels)\n        \n        hd_metric(y_pred=val_outputs, y=val_labels)\n        hd_metric_batch(y_pred=val_outputs, y=val_labels)\n        \n        meanIoU_metric(y_pred=val_outputs, y=val_labels)\n        meanIoU_metric_batch(y_pred=val_outputs, y=val_labels)\n        \n        sd_metric(y_pred=val_outputs, y=val_labels)\n        sd_metric_batch(y_pred=val_outputs, y=val_labels)\n        \n        surfaceDice_metric(y_pred=val_outputs, y=val_labels)\n        surfaceDice_metric_batch(y_pred=val_outputs, y=val_labels)\n    print(\"infer_time: \", time.time()-infer_start)\n\n    metric_org = dice_metric.aggregate().item()\n    metric_batch_org = dice_metric_batch.aggregate()\n    \n    hd_metric_org = hd_metric.aggregate().item()\n    hd_metric_batch_org = hd_metric_batch.aggregate()\n    \n    meanIoU_metric_org = meanIoU_metric.aggregate().item()\n    meanIoU_metric_batch_org = meanIoU_metric_batch.aggregate()\n    \n    sd_metric_org = sd_metric.aggregate().item()\n    sd_metric_batch_org = sd_metric_batch.aggregate()\n    \n    surfaceDice_metric_org = surfaceDice_metric.aggregate().item()\n    surfaceDice_metric_batch_org = surfaceDice_metric_batch.aggregate()\n\n    dice_metric.reset()\n    dice_metric_batch.reset()\n    \n    hd_metric.reset()\n    hd_metric_batch.reset()\n    \n    meanIoU_metric.reset()\n    meanIoU_metric_batch.reset()\n    \n    sd_metric.reset()\n    sd_metric_batch.reset()\n    \n    surfaceDice_metric.reset()\n    surfaceDice_metric_batch.reset()\n\nmetric_tc, metric_wt, metric_et = metric_batch_org[0].item(), metric_batch_org[1].item(), metric_batch_org[2].item()\n\nhd_metric_tc, hd_metric_wt, hd_metric_et = hd_metric_batch_org[0].item(), hd_metric_batch_org[1].item(), hd_metric_batch_org[2].item()\n\nmeanIoU_metric_tc, meanIoU_metric_wt, meanIoU_metric_et = meanIoU_metric_batch_org[0].item(), meanIoU_metric_batch_org[1].item(), meanIoU_metric_batch_org[2].item()\n\nsd_metric_tc, sd_metric_wt, sd_metric_et = sd_metric_batch_org[0].item(), sd_metric_batch_org[1].item(), sd_metric_batch_org[2].item()\n\nsurfaceDice_metric_tc, surfaceDice_metric_wt, surfaceDice_metric_et = surfaceDice_metric_batch_org[0].item(), surfaceDice_metric_batch_org[1].item(), surfaceDice_metric_batch_org[2].item()\n\nprint(\"Metric on original image spacing: \", metric_org)\nprint(f\"metric_tc: {metric_tc:.4f}\")\nprint(f\"metric_wt: {metric_wt:.4f}\")\nprint(f\"metric_et: {metric_et:.4f}\")\n\nprint(\"HD Metric on original image spacing: \", hd_metric_org)\nprint(f\"HD metric_tc: {hd_metric_tc:.4f}\")\nprint(f\"HD metric_wt: {hd_metric_wt:.4f}\")\nprint(f\"HD metric_et: {hd_metric_et:.4f}\")\n\nprint(\"MeanIoU Metric on original image spacing: \", meanIoU_metric_org)\nprint(f\"MeanIoU metric_tc: {meanIoU_metric_tc:.4f}\")\nprint(f\"MeanIoU metric_wt: {meanIoU_metric_wt:.4f}\")\nprint(f\"MeanIoU metric_et: {meanIoU_metric_et:.4f}\")\n\nprint(\"SD Metric on original image spacing: \", sd_metric_org)\nprint(f\"SD metric_tc: {sd_metric_tc:.4f}\")\nprint(f\"SD metric_wt: {sd_metric_wt:.4f}\")\nprint(f\"SD metric_et: {sd_metric_et:.4f}\")\n\nprint(\"Surface Dice Metric on original image spacing: \", surfaceDice_metric_org)\nprint(f\"Surface Dice metric_tc: {surfaceDice_metric_tc:.4f}\")\nprint(f\"Surface Dice metric_wt: {surfaceDice_metric_wt:.4f}\")\nprint(f\"Surface Dice metric_et: {surfaceDice_metric_et:.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cleanup data directory\n\nRemove directory if a temporary was used.","metadata":{}},{"cell_type":"code","source":"if directory is None:\n    shutil.rmtree(root_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}